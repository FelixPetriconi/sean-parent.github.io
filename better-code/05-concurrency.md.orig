---
title: Concurrency
tagline: No Raw Synchronization Primitives

layout: book-page
tags: [ better-code ]
---

### Motivation

{::comment}
For this section I need to first provide motivation for concurrency, and define concurrency and parallelism. which are not commonly understood. Do I need to provide a motivation section for each chapter?
{:/comment}

_Concurrency_ is when multiple tasks start, run, and complete in overlapping time periods and should not be confused with _parallelism_ which is when multiple tasks execute simultaneously. Parallelism requires some form of hardware support, where as concurrency can be achieved strictly through software, such as a cooperative tasking system.

There are two primary benefits for concurrent code. The first is performance by enabling parallelism. The second is to improve interactivity by not blocking the user while a prior action is being processed.

As clock rates on systems have stagnated, hardware developers have turned to parallelism to increase performance. Figure [xxx] shows the performance distribution on a typical desktop system. A single threaded, non-vectorized, application can only utilize about 0.25% of the performance capabilities of the machine.

### Definition of _raw synchronization primitives_.

A _raw synchronization primitive_ is a low level construct used to synchronize access to data. Examples include locks and mutexes, condition variables, semaphores, atomic operations, and memory fences.

{::comment} Discuss difference between data parallelism and task concurrency, so far this chapter is only dealing with tasking. However, it could be expanded upon. {:/comment}

The goal of this chapter is to develop concurrent code without using raw synchronization primitives.

The first problem with raw synchronization primitives are that they are exceedingly error prone to use because, by definition, they require reasoning about non-local effects.

For example, [xxxx] is a snippet from a copy-on-write datatype, this is a simplified version of code from a shipping system.

{::comment}
Insert bad cow example here. Can this example be simplified even more? Remove the template and make it a string?
{:/comment}

The highlighted lines {::comment} how? {:/comment} contain a subtle race condition. The `if` statement is checking the value of an atomic count to see if it is `1`. The `else` statement handles the case where it is not 1. Within the else statement the count is decremented. The problem is that if decrementing the count results in a value of `0` then the object stored in `object_m` should be deleted. The code fails to check for this case, and so an object may be leaked.

The initial test to see if the count was `1` isn't sufficient, between that check and when the count is decremented another thread may have released ownership and decremented the count leaving this object instance as the sole owner.

The fix is to test atomically with the decrement, the correct code is shown in [xxxx].

Another problem with raw synchronization primitives is that their use can have a large negative impact on system performance. To understand why, we need to understand Amdahl's Law.

<<<<<<< Updated upstream
The intuition behind Amdahl's Law is that if part of system takes time x to complete,

{::comment}
Math experiment for Fibonacci matrix.
{:/comment}
$$
\begin{align*}
  \left[ \begin{array}{cc}
      1 & 1 \\
      1 & 0
    \end{array} \right]^{n} =
    \left[ \begin{array}{cc}
      F_{n+1} & F_n \\
      F_n & F_{n-1}
    \end{array} \right]
\end{align*}
$$
=======
The intuition behind Amdahl's Law is that if a part of system takes time x to complete on a single core or processor, then it will encounter a speedup of y if it is run on y cores, but only if no synchronization takes places between the different cores or processors. 
$$S(N) = \frac{1}{(1-P)+\frac{P}{N}}$$
Where the speedup $$S$$ is defined by this equation. $$P$$ is hereby the amount of synchronization in the range of $$[0 .. 1]$$ and $$N$$ the number of cores or processors.

Drawing the abscissa in logarithmic scale illustrates that there is only a speedup of 20 times, even when the system is running on 2048 or more cores and just 5% synchronization takes place.

{::comment} Convert/Create new SVG image
{% include figure.md name='amdahl_log' caption="Amdahl's Law logarithmic scale" %}
{::/comment}

![Amdahl's Law](figures/amdahl_log.png){:height="40%" width="40%"}
Amdahl's Law Logarithmic Scale

Since most desktop or mobile processors have less than 64 cores, it is better to take a look at the graph with linear scale. Each line below the diagonal represents 10% more of serialisation. So if the application just has 10% of serialisation and it is running on 16 cores then the speed-up is just a little better than factor of six.

{::comment} Convert/Create new SVG image
{% include figure.md name='amdahl_lin' caption="Amdahl's Law linear scale" %}
{::/comment}
![Amdahl's Law](figures/amdahl_lin.png){:height="40%" width="40%"}
Amdahl's Law Linear Scale

So Amdahl's law has a huge impact. Serialization doesn't mean only locking on a mutex. Serialization can mean sharing the same memory or sharing the same address bus for the memory, if it is not a NUMA architecture. Sharing the same cache line or anything that is shared within the processor starts to bend that curve down and it bends down rapidly, even an atomic bends that curve down.

An often used model for implementing exclusive access to an object by multiple threads is this:

{% include figure.md name='05-traditional_locking' caption="Exclusive access by different threads" %}

As long as one thread has exclusive access to the object all other threads have to wait until they get the access right. 

This is a horrible way to think about threading. The goal has to be to minimize waiting at all costs. David Butenhof, one of the POSIX implementors, coined the phrase that mutex should be better named bottleneck, because of the property of slowing down an application[^butenhof].

In the following, let's take a look at a traditional piece of code:

{% include code.md name='05-registry-0' caption='Registry Example' %}

It is a registry class with a shared `set` and `get` function. The access to the underlying unordered map is protected against concurrent access with a mutex. At the first glance it seems that only minimal work is done under the mutex. The unordered map is a fairly efficient data structure, it is a hash map. But the amount of time it takes to hash the key depends on the length of the string. So the work that is being done under the lock here is actually fairly unbounded. It depends completely on the lengths of the string. It may be probably typically small but it could be big. On top of the hash calculation comes a potentially allocation of a new bucket within the unordered map, which in most cases requires another lock within the memory manager. This lock can be, depending on the operating system, a global lock within the process.

For a better understanding what shall be actually achieved by using the locks it is necessary to take step back. The C++ standard states here: _It can be shown that programs that correctly use mutexes and memory\_order\_seq\_cst operations to prevent all data races and use no other synchronization operations behave as if the operations executed by their constituent threads were simply interleaved, with each value computation of an object being taken from the last side effect on that object in that interleaving. This is normally referred to as ‘sequential consistency.’_, C++11 Standard 1.10.21.

So why is this an important sentence? It means that one can always think about mutexes as if one has some set of interleaved operations. 

{% include figure.md name='05-sequential_operations' caption='Sequential Operations' %}

* A mutex serializes a set of operations, $$Op_n$$, where the operation is the code executed while the mutex is locked
* Operations are interleaved and may be executed in any order and may be repeated
* Each operation takes an argument, $$x$$, which is the set of all objects mutated under all operations
  * $$x$$ may not be safely read or written without holding the lock if it may be modified by a task holding the lock
* Each operation may yield a result, $$r_m$$, which can communicate information about the state of $$x$$ while it’s associated operation was executed
* The same is true of all atomic operations

So there is not a lot of difference between an `std::atomic`. In fact there is a call on `std::atomic` that returns `true`, if it is lock free. This means the processor supports to do that as an atomic item within the processor or is there not processor support and the compiler has to generate a mutex pair to lock, make the change on the atomic operation, and do the unlock. So all that mutexes and locks are the way to construct atomic operations. 

That means that any piece of code that has a mutex can be transformed into a queued model. This idea applied to the registry example from above leads to this:

{% include code.md name='05-registry-1' caption='registry with queue' %}

Given that there is a serial queue `_q` with an `async` function which executes the passed item and it uses the same calling conventions as `std::async`. Hereby with the difference that it guarantees that the next item being processed in that queue doesn't start until the previous one is completed. Then one can rewrite the `set` string function to be executed with `_q.async`.
As well one can rewrite the `get` string operation. But here the difference is, that one needs the result back out, paired with that particular `get`. It is realized here with a future. (Futures will be covered later in more detail.) So the result of the `get` function, e.g. with a continuation, can be used whenever the `key` is hashed and and the lookup in the hash is completed.

{% include code.md name='05-registry-2' caption='Enhanced Registry with Queue' %}

Why is it important to understand this concept? Because at any place with a mutex in the code one can always make this transformation. One can always transform it into a serialized queue model. And this means that within the serialized queue model anytime somebody can come along and calls `set`, regardless of the amount of work that `set` takes, the time it takes for `set` to return back to the caller itself constant. This means as well that one can add something like an arbitrary `set`, e.g a whole vector of key value pairs. And to the caller this `set` will take just as much time as the previous `set`. It's a non blocking operation with an upper bound of overhead. 

### Definition of _raw threads_

A _thread_ is a execution environment consisting of a stack and processor state running in parallel to other threads.
A _task_ is a unit of work, often a function, to be executed on a thread.

### Problems of _raw threads_

Another common scenario is that increased work within an application is outsourced to a spawned background thread with the intent that the available CPU cores are better utililized. 

{% include figure.md name='05-background_thread' caption='Background Thread executing Tasks' %}

Since this is recognized as a successful idiom to solve performance problems of an application, it becomes easily the default way to solve such issues. 

{% include code.md name='05-background_worker' caption='Simplified Background Worker' %}

Over time the application gets enhanced with more modules and plugins. When now for each of these the same idea was applied then the complete application uses a huge number threads.
An over subscription of threads is then easily the case. That means that more threads are used than CPU cores are available. So the kernel of the operating system has to constantly switch the threads between the available cores to prevent starvation of single threads. 
Within such a switch - named context switch - the CPU registers, program counter and stack pointer of the old thread are saved and the ones from the new thread needs to be restored. This save and restore takes time that is lost for computational tasks of an application. Beside this the translation lookaside buffer (TLB) must be flushed and the page table of the next process is loaded. The flushing of the TLB causes that the memory access of the new thread is slower in the beginning. This causes an additional slow down.
So the goal has to be that the number of context switches is as low as possible.

One way to archive this goal is to use a task system. A task system uses a set of threads, normally equal to the number of CPU cores and distributes the submitted tasks over the available threads. In case that more tasks are submitted than free threads are available then they are put into a queue and whenever one is done the next task is taken from the queue and executed.

{% include figure.md name='05-simple_tasking_system' caption='Simple Tasking System' %}

Since the number of threads is constant, ideally there is no need to perform any context switches. (Because of simplification the fact here is neglected that other system services have running threads as well.) So a task system within an application is an appropriate measure to reduce the number of context switches as long as all modules within it use the same instance of the task system.

For illustrational purpose and understanding better the implications within such a task system, its code is developed in the following. 

The figure above shows that the task system consist out of a notification queue:

{% include code.md name='05-notification_queue-1' caption='Notification Queue' %}

This notification queue is build out of a `deque` of `std::function` with a `mutex` and a `condition_variable`. It has a pop operation which is just going to pull one item off of the queue. And it has a push operation to push one item into the queue and notify anybody who might be waiting on the queue.

{% include code.md name='05-task_system-1' caption='Task System' %}

The task system has a `count` member which contains the number of available cores. It has a vector of threads and the notification queue. The `run` function is the function that will be executed by the threads. In which it creates an empty function object and pops the function from the queue and executes it.
The constructor of the task system spins up one thread and binds it with a lambda against our run function.
When the task system gets destructed, it is necessasry to join all threads. The function that is used by the outside is `async`. It just takes a function and pushs it into the queue.
This system is so far very primitive. As well it would hang on destruction. The latter is corrected by the following additions:

{% include code.md name='05-notification_queue-2' caption='Notification Queue With Termination Switch' %}

{% include code.md name='05-task_system-2' caption='Non-blocking Task System on Destruction' %}

So with the new `done` function the new member `_done` is set and the queue is notified about the change. In case the code is waiting in the pop function, it is woken up from the condition variable and it is checked if `_done` is set and then returned false.

so how do you guys think this performs I already told you right right badly kind of barely get off the mark horrible why is this horrible makes a good example right it's easy to write nothing fundamentally wrong about it it's exactly this model right we've got a single Q and we've got a bunch of threads and they're banging on that Q okay and we've got locks understanding that I can transform those locks into a queue model doesn't help me because all I have is a Q it locks around it okay so I need to come up with a better way.

{% include figure.md name='05-task_system_multiple_queues' caption='Task System with Multiple Notification Queues' %}

{% include code.md name='05-notification_queue-3' caption='Notification Queue With Termination Switch' %}

{% include code.md name='05-task_system-3' caption='Non-blocking Task System on Destruction' %}

{% include figure.md name='05-task_system_task_stealing' caption='Task System with Task Stealing' %}

okay so that's X so here is the common answer I put a Q on each thread that minimizes my contention okay so let's write that we can do this pretty quickly we're just going to have a vector of notification Q's we get to reuse or same notification q okay one for each thread so now our run is going to take an index into into which q it's actually bound against when we join now we need to tell all the Q's they're done next one and when we're going to do our push we're going to keep a little atomic index so we can be pushing from multiple threads and we're just going to keep incrementing it but then we're going to modulo it by our count and so we're just going to round-robin going through
pushing into into it there okay and if you notice that's unsigned so wrapping
around on that index is not a problem well-defined behavior on our overflow
it's a modulo arithmetic okay so how do you guys think we did on our speedometer twice as fast is it going to go half halfway up we're just going to go nobody so so I actually did profile all this code I wrote this all profiled it it's about ten times better okay barely get this movie okay so and we have two problems here one is is I can get a long-running task in one of my Q's and it can cause a bubble and I can have my other course sitting there idle okay so the other is is even in a fully loaded system I can end up with a fair amount of contention right where one of those mutexes blocks and that's you know upwards of a million cycles to do a full task switch right in overhead so having keeping one of those mutexes and actually having it locked kills performance okay so here's the next solution right called task stealing okay now there are lots of sophisticated algorithms that are heavily tuned for
doing task stealing but we're just going to code it very simply here and the idea with task stealing is is if your queue you being a thread is busy because
somebody's pushing in the other end okay ah or it's empty either you don't really care see if you can go pull something out to somebody else's queue
okay so let's do that so and we're going to take our pop and we're going to have tri pop here okay so our tri pop is going to take a tri lock on our mutex
okay so it's going to try to lock will either get the lock or not if we don't
get the lock we just return false okay or if the queue is empty we return false
by the way so we'll do that we're going to do the same thing on our push just for good measure okay so if I'm trying to push something into a queue and that queue is busy I shouldn't wait on it okay cause hold a mutex cause a whole process or cast square to even do a spin lock okay what I'm going to do is if it's busy I'm just going to return false okay couldn't push it okay so now our
caste system our constructor and destructor stay the same okay but when we do the run now what we're going to do is we're going to do try pop and we'll just run once round all the queues okay and then if we don't get anything once around then we'll just wait where we were okay now on the push side we'll spin around and it turns out on the push if all the cues are busy right right you have to push okay so so what are you going to do well you could just sit and wait on one cue but there's no guarantee that that's the first cue that frees up okay so it's actually better on the push to spin a little more okay so I've got a que there right right and there's a trade-off there you're you're improving overall throughput of the system versus sacrificing a single core to burn it in the spin loop so whether or not it's a win in your system depends on what the load is on your system okay whether or not you have that core to burn so but you can tune it quite a bit so now how do you think we did
okay so way better okay so we got way up into there now at this point we're about a hundred times better than when we started from Oracle's example code so
this is a pretty huge improvement okay now what I was benchmarking these at was
Apple's Grand Central Dispatch that's my one point oh right that's my mindful thing apples Grand Central Dispatch is a very nice piece of work if you actually go and read the code and it even has hooked into Lib colonel and the reason why it has looks into Lib colonel is because it understands when one task is blocked and how to go just ignore that task and go take care of other
things so you cannot match it with the primitives that you have in standard C++
okay but we actually came within about 15% okay of it with some straightforward
code so that's pretty good
























But as soon as each application on a system uses its own thread pool there is again the problem of over subscription.

So the only solution to reduce the amount of context switches is that all application within a system use the same thread pool. Since an optimized thread pool needs knowledge about the locking state of a possible task within the thread pool to potentially spawn new threads to avoid dead locks this must be an operating system facility. MacOS and Windows provide here out of the box a thread pool through a low level API.



### Problems of call backs
* Contra: Hard dto reason about the flow in the code
* Contra: Callback must be set beforehand, futures can be attached later
* Pro: Can be faster than futures because no overhead of shared state counter


#### Futures as abstraction

Futures
Conceptually, a future is a way to separate the result of a function from the execution of the function. The task (the function packaged so it returns void) can be executed in a different context (the execution context is controlled by executors in some of the proposals) and the result will become available via the future.

A future also serves as a handle to the associated task, and may provide some operation to control the task.

The primary advantage of a future over a simple callback is that a callback requires you to provide the subsequent operation in advance. Where a future allows you to provide a continuation, via a then() member function, at some later point. This feature makes futures easier to compose, easier to integrate into an existing system, and more powerful as they can be stored and the continuation can be attached as the result of another action, later. However, this flexibility comes with inherent cost, it requires an atomic test when the continuation is attached to determine if the value is already available. Because of this cost, for many library operations it makes sense to provide a form taking a callback as well as one returning a future. Although at first glance it may appear that a callback from is easily adapted to a future form, that is not the case for reasons discussed below.

#### Channels (or actors) as abstraction




### Motivation

1st example export from ui with compression and possibility to cancel
2nd example export group of images with compression and possibility to cancel

### Develop Solution

1st solution Use futures 
2nd solution Use channels

### Conclusion

[^cow_definition]:
    Copy-on-write [https://en.wikipedia.org/wiki/Copy-on-write](https://en.wikipedia.org/wiki/Copy-on-write)

[^cow]:
    Copy-on-write implementation in stlab. [https://github.com/stlab/libraries/blob/develop/stlab/copy_on_write.hpp](https://github.com/stlab/libraries/blob/develop/stlab/copy_on_write.hpp)

[^butenhof]:
    Recursive mutexes by David Butenhof [http://zaval.org/resources/library/butenhof1.html](Recursive mutexes by David Butenhof)
>>>>>>> Stashed changes
